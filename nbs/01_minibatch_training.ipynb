{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch import tensor,nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
    "np.set_printoptions(precision=2, linewidth=125)\n",
    "\n",
    "MNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\n",
    "path_data = Path('data')\n",
    "path_data.mkdir(exist_ok=True)\n",
    "path_gz = path_data / 'mnist.pkl.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "if not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = x_train.shape\n",
    "c = y_train.max() + 1\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.15,  0.15, -0.09,  ...,  0.10, -0.05,  0.19],\n",
       "        [-0.18,  0.19, -0.03,  ...,  0.06, -0.05,  0.21],\n",
       "        [-0.01,  0.10, -0.03,  ...,  0.13, -0.02,  0.13],\n",
       "        ...,\n",
       "        [-0.11,  0.14, -0.08,  ...,  0.12, -0.13,  0.27],\n",
       "        [-0.09,  0.13,  0.02,  ...,  0.11, -0.04,  0.29],\n",
       "        [-0.11,  0.20, -0.07,  ...,  0.23, -0.09,  0.16]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(m, nh, 10)\n",
    "preds = model(x_train)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsoftmax(x): return x - x.exp().sum(-1, keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.49, -2.19, -2.43,  ..., -2.24, -2.39, -2.15],\n",
       "        [-2.54, -2.17, -2.38,  ..., -2.30, -2.40, -2.14],\n",
       "        [-2.36, -2.25, -2.38,  ..., -2.22, -2.37, -2.22],\n",
       "        ...,\n",
       "        [-2.47, -2.22, -2.43,  ..., -2.24, -2.49, -2.09],\n",
       "        [-2.46, -2.24, -2.35,  ..., -2.26, -2.41, -2.08],\n",
       "        [-2.46, -2.15, -2.43,  ..., -2.12, -2.45, -2.19]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logsoftmax(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    x = x.max(-1)[0]\n",
    "    return m + (x - m[:, None]).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.logsumexp(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.49, -2.19, -2.43,  ..., -2.24, -2.39, -2.15],\n",
       "        [-2.54, -2.17, -2.38,  ..., -2.30, -2.40, -2.14],\n",
       "        [-2.36, -2.25, -2.38,  ..., -2.22, -2.37, -2.22],\n",
       "        ...,\n",
       "        [-2.47, -2.22, -2.43,  ..., -2.24, -2.49, -2.09],\n",
       "        [-2.46, -2.24, -2.35,  ..., -2.26, -2.41, -2.08],\n",
       "        [-2.46, -2.15, -2.43,  ..., -2.12, -2.45, -2.19]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred = log_softmax(preds)\n",
    "sm_pred\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integer array indexing\n",
    "\n",
    "- allows selection of items in an array based on their N-dimensional index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.27, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.54, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.16, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[0,5], sm_pred[1, 0], sm_pred[2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.27, -2.54, -2.16], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[[0, 1, 2], y_train[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target): return -input[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.32, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll(sm_pred, y_train)\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pytorch, `F.log_softmax` and `F.nll_loss` are combined in one optimized function, `F.cross_entropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.32, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(preds, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Training Loop\n",
    "\n",
    "A training loop repeats over the following steps:\n",
    "- get the output of the model on a batch of input.\n",
    "- compare the output to the labels we have and compute the loss.\n",
    "- calculate the gradients of the loss with respect to every parameter of the model.\n",
    "- update said parameters with those gradients to make them a little bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.15,  0.15, -0.09, -0.05,  0.21,  0.07, -0.08,  0.10, -0.05,  0.19], grad_fn=<SelectBackward0>),\n",
       " torch.Size([50, 10]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 50\n",
    "\n",
    "xb = x_train[0:bs]\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7,\n",
       "        6, 1, 8, 7, 9, 3, 9, 8, 5, 9, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[:bs]\n",
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.31, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 9, 4, 4, 9, 5, 4, 9, 9, 4, 9, 5, 4, 4, 9, 9, 4, 9, 9, 4, 4, 9, 9, 4, 9, 4, 4, 1, 9, 9, 1, 9, 9, 9, 9, 9, 7, 9, 4,\n",
       "        9, 9, 4, 9, 9, 9, 9, 1, 9, 9, 9])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def accuracy(out, yb): return (out.argmax(dim=1) == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.22)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def report(loss, preds, yb): print(f'{loss:.2f}, {accuracy(preds, yb):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.31, 0.22\n"
     ]
    }
   ],
   "source": [
    "xb, yb = x_train[:bs], y_train[:bs]\n",
    "preds = model(xb)\n",
    "report(loss_func(preds, yb), preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13, 0.96\n",
      "0.08, 0.98\n",
      "0.08, 0.98\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n, i+bs))\n",
    "        xb, yb = x_train[s], y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, 'weight'):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias -= l.bias.grad * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Parameters and optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Module(\n",
       "  (foo): Linear(in_features=3, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = nn.Module()\n",
    "m1.foo = nn.Linear(3, 4)\n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('foo', Linear(in_features=3, out_features=4, bias=True))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(m1.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_children at 0x0000019A44D51CB0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.named_children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.07,  0.04, -0.15],\n",
       "         [-0.20, -0.12,  0.10],\n",
       "         [ 0.31,  0.31, -0.36],\n",
       "         [-0.33,  0.35, -0.07]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.11,  0.57, -0.13,  0.01], requires_grad=True)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(m1.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x): return self.l2(self.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=50, bias=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(m, nh, 10)\n",
    "model.l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1: Linear(in_features=784, out_features=50, bias=True)\n",
      "l2: Linear(in_features=50, out_features=10, bias=True)\n",
      "relu: ReLU()\n"
     ]
    }
   ],
   "source": [
    "for name, l in model.named_children(): print(f'{name}: {l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, n, bs):\n",
    "            s = slice(i, min(n, i+bs))\n",
    "            xb, yb = x_train[s], y_train[s]\n",
    "            preds = model(xb)\n",
    "            loss = loss_func(preds, yb)\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters(): p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "        report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14, 0.96\n",
      "0.09, 0.96\n",
      "0.05, 0.98\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule:\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self._modules = {}\n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "        \n",
    "    def __setattr__(self, k, v):\n",
    "        if not k.startswith('_'): self._modules[k] = v\n",
    "        super().__setattr__(k, v)\n",
    "        \n",
    "    def __repr__(self): return f'{self._modules}'\n",
    "    \n",
    "    def parameters(self):\n",
    "        for l in self._modules.values(): yield from l.parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MyModule class\n",
    "\n",
    "`__init__` Method:\n",
    "- `self._modules` is a dict used to store references to the submodules with `MyModule`.\n",
    "- `self.l1` and `self.l2` are instances of the linear layers of the neural network.\n",
    "\n",
    "`__setattr__` Method:\n",
    "- it is overridden to customize attribute assignment behaviour. Whenever an attribute is set on an instance of `MyModule`, this method is called.\n",
    "- Here, it stores the assigned attribute('v') in the '_modules' dict if the attribute name('k') doesn't start with an '_'.\n",
    "- it then calls the parent class's `__setattr__` method to perform the actual assignment.\n",
    "\n",
    "`parameters` Method:\n",
    "- It iterates over the valies of the '_modules' dict and yields the parameters of each submodule using the 'parameters()' method.\n",
    "- The 'yield from' statement allows the generator to yield directly from the nested generator produced by 'l.parameters'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = MyModule(m, nh, 10)\n",
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in mdl.parameters(): print(p.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i, l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n",
    "        \n",
    "    def forward(self, x): return reduce(lambda val, layer: layer(val), self.layers, x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model(nn.Module)\n",
    "\n",
    "`__init__` method:\n",
    "- For each layer, it adds the layer as a submodule to the `Model` using `self.add_module(f'layer_{i}', l)`. This allows the layer to be recognized and handles properly by PyTorch.\n",
    "\n",
    "`forward` method:\n",
    "- defines the forward pass computations of the `Model` and takes an input tensor `x` as the input.\n",
    "- It used the `reduce` function to iteratively apply the layers to the input tensor `x`.\n",
    "- For each layer in `self.layers`, it applies the layer to the current value(`val`) using the lambda function `lambda val, layer: layer(val)`.\n",
    "- The output of each layer becomes the input to the next layer in the iteration.\n",
    "- The final output of the forward pass is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer_1): ReLU()\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 10])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xb).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.ModuleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SequentialModel(layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13, 0.98\n",
      "0.09, 0.98\n",
      "0.08, 0.96\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13, 0.98\n",
      "0.09, 0.96\n",
      "0.11, 0.98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.04, grad_fn=<NllLossBackward0>), tensor(1.))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5): self.params, self.lr = list(params), lr\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * self.lr\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.params: p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19, 0.96\n",
      "0.17, 0.94\n",
      "0.08, 0.96\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n, i+bs))\n",
    "        xb, yb = x_train[s], y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.30, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "loss_func(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09, 0.98\n",
      "0.05, 0.98\n",
      "0.03, 1.00\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n, i+bs))\n",
    "        xb, yb = x_train[s], y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x, self.y = x, y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "assert len(train_ds) == len(x_train)\n",
    "assert len(valid_ds) == len(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = train_ds[:5]\n",
    "assert xb.shape == (5, 28*28)\n",
    "assert yb.shape == (5,)\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21, 0.92\n",
      "0.11, 0.96\n",
      "0.08, 0.98\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        xb, yb = train_ds[i:min(n, i+bs)]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, bs): self.ds, self.bs = ds, bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i + self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs)\n",
    "valid_dl = DataLoader(valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 784])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 8, 6, 9, 6, 4, 5, 3, 8, 4, 5, 2, 3, 8, 4, 8, 1, 5, 0, 5, 9, 7, 4, 1, 0, 3, 0, 6, 2, 9, 9, 4, 1, 3, 6, 8, 0, 7, 7,\n",
       "        6, 8, 9, 0, 3, 8, 3, 7, 7, 8, 4])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[0].view(28, 28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            preds = model(xb)\n",
    "            loss = loss_func(preds, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16, 0.98\n",
      "0.12, 0.96\n",
      "0.13, 0.96\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.04, grad_fn=<NllLossBackward0>), tensor(0.98))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, shuffle=False): self.n, self.shuffle = len(ds), shuffle\n",
    "    def __iter__(self):\n",
    "        res = list(range(self.n))\n",
    "        if self.shuffle: random.shuffle(res)\n",
    "        return iter(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = Sampler(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "it = iter(ss)\n",
    "for o in range(5): print(next(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(islice(ss, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32856, 46404, 44381, 25721, 10721]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = Sampler(train_ds, shuffle=True)\n",
    "list(islice(ss, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcore.all as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler():\n",
    "    def __init__(self, sampler, bs, drop_last=False): fc.store_attr()\n",
    "    def __iter__(self): yield from fc.chunked(iter(self.sampler), self.bs, drop_last=self.drop_last)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BatchSampler()\n",
    "\n",
    "`__init__` method:\n",
    "- it takes argument: `sampler`, which is the underlying sampler object used to sampler elements for each batch; `bs`, batch size; `drop_last`, boolean flag indicating whether to drop last batch if it is smaller than the specified batch size\n",
    "- `fc.store_attr` is a shorthand way of providing arguments as instance attributes of the `BatchSampler` object.\n",
    "\n",
    "`__iter__` method:\n",
    "- it calls the `fc.chunked` function to chunk the iterator into batches.\n",
    "- `fc.chunked` takes the arguments: `iter(self.sampler)` iterator to create batches from; `self.bs`, specifying number of elements per batch; `drop_last` indicating whether to drop the last batch.\n",
    "- `yield from` is used to yield each batch from the iterator, effectively yielding batches of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[45326, 29345, 39038, 46924],\n",
       " [27675, 26277, 24586, 11654],\n",
       " [14679, 49047, 38049, 21950],\n",
       " [8868, 12100, 15289, 154],\n",
       " [10405, 41234, 31653, 27549]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchs = BatchSampler(ss, 4)\n",
    "list(islice(batchs, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    xs, ys = zip(*b)\n",
    "    return torch.stack(xs), torch.stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, batchs, collate_fn=collate): fc.store_attr()\n",
    "    def __iter__(self): yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = BatchSampler(Sampler(train_ds, shuffle=True), bs)\n",
    "valid_samp = BatchSampler(Sampler(valid_ds, shuffle=False), bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batchs=train_samp)\n",
    "valid_dl = DataLoader(valid_ds, batchs=valid_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "plt.imshow(xb[0].view(28, 28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 784]), torch.Size([50]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27, 0.90\n",
      "0.08, 0.96\n",
      "0.09, 0.98\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from fastcore.basics import store_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([1, 1, 1, 0]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[[3, 6, 8, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([1, 1, 1, 0]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.__getitem__([3, 6, 8, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([1, 1, 1, 0]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.__getitem__([3, 6, 8, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1]))\n",
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 0]))\n"
     ]
    }
   ],
   "source": [
    "for o in map(train_ds.__getitem__, ([3,6], [8, 1])): print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, batchs, n_workers=1, collate_fn=collate): fc.store_attr()\n",
    "    def __iter__(self):\n",
    "        with mp.Pool(self.n_workers) as ex: yield from ex.map(self.ds.__getitem__, iter(self.batchs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__iter__` method of DataLoader:\n",
    "- it creates a `mp.Pool` object with `self.n_workers` worker processes using the `with` statement. `mp.Pool` is used to parallelize the data loading process.\n",
    "- `ex.map` function is used to apply the `self.ds.__getitem__` method to each item in the iterable `iter(self.batchs)`.\n",
    "    - `self.ds` is the dataset object.\n",
    "    - `self.ds.__getitem__` is called for each item in `item(self.bacths)`, which returns the corresponding data sample.\n",
    "    - `ex.map` function applies this mapping operation in parallel using the worker processes in the pool\n",
    "- `yield from` is used to yield each data sample obtained from `ex.map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batchs=train_samp, n_workers=2)\n",
    "it = iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(it)\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = BatchSampler(RandomSampler(train_ds), bs, drop_last=False)\n",
    "valid_samp = BatchSampler(SequentialSampler(valid_ds), bs, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_sampler=train_samp, collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, batch_sampler=valid_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07, 0.96\n",
      "0.05, 1.00\n",
      "0.09, 0.98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.04, grad_fn=<NllLossBackward0>), tensor(0.98))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True)\n",
    "valid_dl = DataLoader(valid_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08, 0.98\n",
      "0.07, 0.98\n",
      "0.03, 1.00\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.03, grad_fn=<NllLossBackward0>), tensor(1.))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([9, 1, 3]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[[4, 6, 7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=train_samp)\n",
    "valid_dl = DataLoader(valid_ds, sampler=valid_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 50, 784]), torch.Size([1, 50]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|epxort\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss, tot_acc, count = 0., 0., 0\n",
    "            for xb, yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                n = len(xb)\n",
    "                count += n\n",
    "                tot_loss += loss_func(pred, yb).item() * n\n",
    "                tot_acc += accuracy(pred, yb).item() * n\n",
    "        print(epoch, tot_loss/count, tot_acc/count)\n",
    "    return tot_loss/count, tot_acc/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_dls(train_ds, valid_ds, bs)\n",
    "model, opt  = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.14687670616433024 0.9559000039100647\n",
      "1 0.1144107170496136 0.9665000057220459\n",
      "2 0.10815925584640354 0.9692000061273575\n",
      "3 0.13109441814711317 0.963900004029274\n",
      "4 0.10893094312748872 0.9684000051021576\n",
      "CPU times: total: 15.1 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%time loss, acc = fit(5, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
